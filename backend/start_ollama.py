#!/usr/bin/env python3
import subprocess
import sys
import time
import os

# Chemin complet vers ollama.exe
OLLAMA_CMD = r"C:\Users\Bellamine Kenza\AppData\Local\Programs\Ollama\ollama.exe"

# Nom du mod√®le que tu as d√©j√† t√©l√©charg√©
MODEL_NAME = "deepseek-r1:7b"

def start_ollama():
    """D√©marre Ollama si ce n'est pas d√©j√† fait et v√©rifie le mod√®le DeepSeek"""
    try:
        # Ajouter Ollama au PATH temporairement (optionnel)
        ollama_dir = os.path.dirname(OLLAMA_CMD)
        os.environ["PATH"] += os.pathsep + ollama_dir

        # V√©rifier si Ollama fonctionne
        result = subprocess.run([OLLAMA_CMD, 'list'], capture_output=True, text=True)
        
        if result.returncode != 0:
            print("üöÄ D√©marrage d'Ollama...")
            subprocess.Popen([OLLAMA_CMD, 'serve'])
            time.sleep(5)
            print("‚úÖ Ollama d√©marr√©")
            result = subprocess.run([OLLAMA_CMD, 'list'], capture_output=True, text=True)

        # V√©rifier si le mod√®le DeepSeek est pr√©sent
        if MODEL_NAME.lower() not in result.stdout.lower():
            print(f"‚ùå Le mod√®le {MODEL_NAME} n'est pas trouv√©.")
            print(f"üëâ Assurez-vous que {MODEL_NAME} est bien t√©l√©charg√© via Ollama.")
            return False

        return True

    except FileNotFoundError:
        print("‚ùå Ollama n'est pas trouv√©.")
        print(f"V√©rifiez le chemin : {OLLAMA_CMD}")
        print("üëâ Installez Ollama si n√©cessaire: https://ollama.ai")
        return False

if __name__ == "__main__":
    if start_ollama():
        print("‚úÖ Tout est pr√™t pour l'assistant IA avec DeepSeek!")
        print("üëâ Lancez maintenant: python main.py")
    else:
        sys.exit(1)
